{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parts of notebook are taken from SBI tutorials and Peter Melchior's LFI introduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to perform inference on model's parameters $\\theta$ given observations $\\{x\\}_i$:\n",
    "$$ P(\\theta | \\{x\\}_i)$$\n",
    "\n",
    "We do this with Bayes rule:\n",
    "$$ P(\\theta | \\{x\\}_i) \\propto P(\\{x\\}_i|\\theta) P(\\theta),$$\n",
    "which relies on the Likelihood function $P(\\{x\\}_i|\\theta)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "However, what if we don't know the Likelihood?\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Well say we have a simulator. Then we can simulate enough examples such that we can measure the frequency of examples reproducing the data (or getting close to it). This is called Approximate Bayesian Computation, or ABC:\n",
    "\n",
    "![ABC](https://upload.wikimedia.org/wikipedia/commons/b/b9/Approximate_Bayesian_computation_conceptual_overview.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, what if the simulation is very expensive, and the data is high dimensional? Then we would need to run far too many simulations. E.g., images of a galaxy - we can't simply simulate enough galaxies until the pixels match up!\n",
    "\n",
    "So we turn to Likelihood-Free Inference. Instead of needing to call the simulation over and over, we instead run it only a few times, and fit a function:\n",
    "\n",
    "$$P(\\theta|x)$$\n",
    "\n",
    "for several examples of $\\{\\theta, x\\}_i$, such that $\\sum_i P(\\theta|x)$ is maximized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch quick start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch is like numpy in syntax, but adds: vectorization, GPU-acceleration, and autodifferentiation. It also has deep learning kits. Let's do some linear algebra:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of `np.array`, we use `torch.tensor`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5., 3.]), tensor([5., 3.]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([5., 3.]), torch.tensor([5., 3.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we can pass data back and forth between numpy and torch like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([10.,  5.,  1.]),\n",
       " tensor([10.,  5.,  1.], dtype=torch.float64),\n",
       " array([10.,  5.,  1.]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([10., 5., 1.])\n",
    "y = torch.tensor(x)\n",
    "z = np.array(y)\n",
    "x, y, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a vector of 10 zeros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros(10)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add 1 to each element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate random numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.2177,  0.0391,  1.3016,  0.7003,  1.0195, -0.2013,  0.4223, -0.8245,\n",
       "        -1.0312,  1.5593])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(10)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do some basic operations, all with numpy syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.3458, 0.9992, 0.2660, 0.7647, 0.5238, 0.9798, 0.9122, 0.6789, 0.5138,\n",
       "         0.0115]),\n",
       " tensor([0.7965, 0.0383, 0.8336, 0.5308, 0.7028, 0.1834, 0.3522, 0.6013, 0.7086,\n",
       "         0.9397]),\n",
       " tensor(1.7673),\n",
       " tensor(0.9882))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cos(x), torch.log(torch.abs(x) + 1), torch.sum(x), x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do stuff on the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5473, -1.4541, -0.4094,  ..., -0.9467, -0.3957, -0.9190],\n",
       "        [ 1.2915,  0.3078,  1.3502,  ..., -0.2240,  1.4586, -1.1972],\n",
       "        [ 0.7421, -1.2721,  0.3409,  ..., -0.6925,  1.2249, -0.8580],\n",
       "        ...,\n",
       "        [-0.6698,  1.7907, -1.3751,  ...,  0.6343, -0.2695,  0.5030],\n",
       "        [-0.1271,  1.6968, -1.8079,  ..., -0.1754, -1.8075, -0.9347],\n",
       "        [-1.1590, -0.4660,  0.1615,  ...,  0.3799,  0.2057, -1.4509]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn((100, 100))\n",
    "\n",
    "#Move to GPU:\n",
    "x = x.cuda()\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the vector is on the GPU. We can do vector operations all on the GPU now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.6822, -2.9593,  0.8478,  ..., -0.2651,  0.8598, -0.1704],\n",
       "        [ 2.4289,  0.9812,  2.6792,  ...,  0.9628,  3.2140, -1.3521],\n",
       "        [ 1.1448, -1.7652,  0.9811,  ...,  0.4366,  2.1759,  0.0214],\n",
       "        ...,\n",
       "        [ 0.4825,  5.5231, -2.4069,  ...,  1.0597,  0.9434,  1.0024],\n",
       "        [ 0.9889,  4.7585, -6.1445,  ...,  0.9783, -6.1405, -0.2235],\n",
       "        [-1.1578,  0.7912,  0.9902,  ...,  0.9826,  0.9866, -2.9355]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x**3 + torch.cos(x) - x.mean()\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And move back to the CPU like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.6822, -2.9593,  0.8478,  ..., -0.2651,  0.8598, -0.1704],\n",
       "        [ 2.4289,  0.9812,  2.6792,  ...,  0.9628,  3.2140, -1.3521],\n",
       "        [ 1.1448, -1.7652,  0.9811,  ...,  0.4366,  2.1759,  0.0214],\n",
       "        ...,\n",
       "        [ 0.4825,  5.5231, -2.4069,  ...,  1.0597,  0.9434,  1.0024],\n",
       "        [ 0.9889,  4.7585, -6.1445,  ...,  0.9783, -6.1405, -0.2235],\n",
       "        [-1.1578,  0.7912,  0.9902,  ...,  0.9826,  0.9866, -2.9355]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = y.cpu()\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch also lets you do autodifferentiation, which is a big part of deep learning. Let's look at the gradient of $\\sum_i \\cos(x_i)^2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_func(x):\n",
    "    return (torch.cos(x)**2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we make our variable know that it should record gradients as it is operated upon with the `requires_grad` flag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([np.pi/4, np.pi/8])\n",
    "x.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we take the gradient at each of $x=({\\pi \\over 4}, {\\pi \\over 8})$, which is -2 \\cos(x) \\sin(x):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-1.0000, -0.7071]),)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.grad(my_func(x), x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do this all on the GPU! We can also do higher-order derivatives by repeatedly calling grad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-1.0000, -0.7071], device='cuda:0'),)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x.cuda()\n",
    "torch.autograd.grad(my_func(y), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With gradient information, one can do gradient descent operation on a model using `torch.optim.SGD`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.optim.SGD;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can write a class that inherits from `torch.nn.Module`, declares `torch.nn.Parameters(...)` around its parameters, and then `torch.optim.SGD` can optimize those. This is how all deep learning works: gradient-based optimization of some highly-flexible differentiable model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch is used as the backend for SBI. Let's move on to that now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Likelihood-free Inference Tutorial:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say that there is the following *true* model---a 2D Gaussian with known width:\n",
    "\n",
    "$$(x_1, x_2) \\sim \\mathcal{N}(\\mu, \\Sigma=0.25 I)$$\n",
    "\n",
    "The true vector for the Gaussian is $\\mu = (3, -1.5)$, which we are unaware of. We make 5 observations and attempt to reconstruct a posterior over $\\mu$.\n",
    "\n",
    "The likelihood of this model is given by $$P(x|\\mu) \\sim \\exp\\left({(x_1-3)^2 + (x_2- (-1.5))^2\\over 2 \\cdot 0.25}\\right),$$\n",
    "which gives us a way to recover the parameter given data and a prior: $P(\\mu|x) \\sim P(x|\\mu) P(\\mu)$.\n",
    "\n",
    "\n",
    "But now, say that **we don't know this likelihood**. Pretend we are not given the likelihood of a Gaussian. We only know how to draw samples from a Gaussian: this is our \"simulation.\"\n",
    "\n",
    "#### How can we compute a distribution over $\\mu$, without a likelihood?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihood-free Inference\n",
    "\n",
    "For LFI, you need to provide two ingredients:\n",
    "\n",
    "1. a prior distribution that allows to sample parameter sets (your guess at $\\mu$)\n",
    "2. a simulator that takes parameter sets and produces simulation outputs (samples of a Gaussian with a given $\\mu$)\n",
    "\n",
    "For example, let's pretend we have a reasonable idea that $\\mu$ is within $[-10, 10]$ with uniform probability. We also have our simulator that generates samples of a Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the inference procedure\n",
    "\n",
    "`sbi` provides a simple interface to run state-of-the-art algorithms for simulation-based inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with `sbi`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sbi.utils as utils\n",
    "from sbi.inference.base import infer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write out our uniform prior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = utils.BoxUniform(\n",
    "    low=torch.tensor([-10., -10.]),\n",
    "    high=torch.tensor([10., 10.])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be used to generate proposals for $\\mu$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's write out our simulator for a given $\\mu$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulator(mu):\n",
    "    # Generate samples from N(mu, sigma=0.5)\n",
    "    \n",
    "    return mu + 0.5 * torch.randn_like(mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it by inputting a vector of potential $\\mu$ values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7272,  6.8405],\n",
       "        [ 3.4974,  5.2752],\n",
       "        [-7.7825, -2.4473]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulator(\n",
    "    torch.tensor([[1, 8.], [3., 5.], [-7., -3.]])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here are samples of each simulation. Now, we will use this simulator with Likelihood-Free Inference to construct a distribution over potential mu values:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Let's learn a likelihood from the simulator:\n",
    "\n",
    "The likelihood is a type of \"normalized\" Neural Network---called a \"normalizing flow\". (as an example check out my paper where we learn an accurate CMD straight from Gaia data with a normalizing flow: https://arxiv.org/abs/1908.08045)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoxUniform(Uniform(low: torch.Size([2]), high: torch.Size([2])), 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be a bit slow because we are training the neural network on a CPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15732c8c64384ec2a4819c0e2882f153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Running 1000 simulations.', max=1000.0, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Neural network successfully converged after 91 epochs.\n"
     ]
    }
   ],
   "source": [
    "posterior = infer(\n",
    "    simulator,\n",
    "    prior,\n",
    "#     method='SNPE',\n",
    "    method='SNLE',\n",
    "    num_workers=-1,\n",
    "    num_simulations=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a neural network that acts as our likelihood! Next, let's record our \"observations\" of the true distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_observations = 10\n",
    "observation = torch.tensor([3., -1.5])[None] + 0.5*torch.randn(n_observations, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.scatterplot(x=observation[:, 0], y=observation[:, 1])\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.axis('scaled')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use our learned likelihood (a neural network) to do inference on $\\mu$! The way the problem is structured on the neural net side, we need to manually include all observations separately. But we can make samples of a single datapoint like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tuning bracket width...: 100%|██████████| 50/50 [00:05<00:00,  9.22it/s]\n",
      "Generating samples: 100%|██████████| 20/20 [00:05<00:00,  3.57it/s]\n",
      "Generating samples:  35%|███▍      | 3460/10000 [16:09<30:28,  3.58it/s]"
     ]
    }
   ],
   "source": [
    "samples = posterior.sample((10000,), x=observation[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then plot this (it should look approximately Gaussian):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log_probability = posterior.log_prob(samples, x=observation[0])\n",
    "out = utils.pairplot(samples, limits=[[-10,10],[-10,10]], fig_size=(6,6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a grid of $\\mu$ close to the expected value of $(3, -15)$, and calculate the total log likelihood for each value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "bounds = [3-1, 3+1, -1.5-1, -1.5+1]\n",
    "\n",
    "mu_1, mu_2 = torch.tensor(np.mgrid[bounds[0]:bounds[1]:2/50., bounds[2]:bounds[3]:2/50.]).float()\n",
    "\n",
    "samples = torch.cat(\n",
    "    (mu_1.reshape(-1, 1), mu_2.reshape(-1, 1)),\n",
    "    dim=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prob = sum([posterior.log_prob(samples, x=observation[i]) for i in range(len(observation))])\n",
    "prob = torch.exp(log_prob)\n",
    "plt.contourf(prob.reshape(*mu_1.shape), extent=bounds, origin='lower')\n",
    "plt.axis('scaled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, this was a simple example and we could have written out an explicit likelihood. But the real power of Likelihood-Free Inference comes for high dimensional and complicated distributions, when there is no explicit analytic likelihood available. The exact same algorithm as above can simply be scaled up to a higher dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare to the true likelihood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_like = lambda x: -((x[0] - mu_1)**2 + (x[1] - mu_2)**2)/(2*0.5**2)\n",
    "log_prob = sum([true_like(observation[i]) for i in range(len(observation))])\n",
    "prob = torch.exp(log_prob)\n",
    "plt.contourf(prob, extent=bounds, origin='lower')\n",
    "plt.axis('scaled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad considering we only used O(1000) examples to learn the likelihood! The more simulations you use (much faster to train with GPUs), the more accurate the likelihood will be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emphasis:\n",
    "\n",
    "LFI is a very general approach that will eventually be used everywhere in astrophysics where we don't have clear  likelihoods. To help give examples of what it could do:\n",
    "\n",
    "- Photo-z: parameter is z, you then generate some SEDs - that is your \"simulator\". Then use `sbi` to compare to data. You get a posterior over z.\n",
    "- Galaxy images: parameters are galaxy properties, orientation, distance, you then simulate some images of galaxies. You then use `sbi` to infer the galaxy properties.\n",
    "    - Here, one would probably want to use an autoencoder to first compress the images to vectors describing the properties.\n",
    "    - The likelihood is then learned over that autoencoded vector.\n",
    "- Cosmological parameters from a density field, simulate some power spectrum and feed into `sbi`. \n",
    "    - To avoid parts of the simulation you know are bad (e.g., small scales), simply add noise to them so the neural net doesn't use them.\n",
    "    - Again, here one would need to create summary statistics using another model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements for the simulator, prior, and observation\n",
    "\n",
    "Regardless of the algorithm you need to provide a prior and a simulator for training. Let's talk about what requirements they need to satisfy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Prior\n",
    "A prior is a distribution object that allows to sample parameter sets. Any class for the prior is allowed as long as it allows to call `prior.sample()` and `prior.log_prob()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulator\n",
    "The simulator is a Python callable that takes in a parameter set and outputs data with some (even if very small) stochasticity.\n",
    "\n",
    "Allowed data types and shapes for input and output:\n",
    "\n",
    "- the input parameter set and the output have to be either a `np.ndarray` or a `torch.Tensor`. \n",
    "- the input parameter set should have either shape `(1,N)` or `(N)`, and the output must have shape `(1,M)` or `(M)`.\n",
    "\n",
    "You can call simulators not written in Python as long as you wrap them in a Python function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "Once you have a trained posterior, you will want to evaluate or sample the posterior $p(\\theta|x_o)$ at certain observed values $x_o$:\n",
    "\n",
    "- The allowable data types are either Numpy `np.ndarray` or a torch `torch.Tensor`.\n",
    "- The shape must be either `(1,M)` or just `(M)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running different algorithms\n",
    "\n",
    "`sbi` implements three classes of algorithms that can be used to obtain the posterior distribution: SNPE, SNLE, and SNRE. You can try the different algorithms by simply swapping out the `method`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = infer(simulator, prior, method='SNPE', num_simulations=1000)\n",
    "posterior = infer(simulator, prior, method='SNLE', num_simulations=1000)\n",
    "posterior = infer(simulator, prior, method='SNRE', num_simulations=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then infer, sample, evaluate, and plot the posterior as described above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main2",
   "language": "python",
   "name": "main2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
